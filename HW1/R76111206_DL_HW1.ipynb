{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vryXqEjDN0-Y"
   },
   "source": [
    "### 讀取TXT檔的Path與Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xF4EkK_C_7O5"
   },
   "outputs": [],
   "source": [
    "\n",
    "y_train=[]\n",
    "train_paths=[]\n",
    "\n",
    "\n",
    "# Read text file containing image paths\n",
    "with open('train.txt', 'r') as f:\n",
    "    for line in f.readlines():  \n",
    "        image_train = line[:-1].split(' ')\n",
    "\n",
    "        train_paths.append(str(image_train[0])) #存path\n",
    "        train_labels = image_train[1]\n",
    "\n",
    "        y_train.append(train_labels) #存label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-E1M8SP9s-yE"
   },
   "outputs": [],
   "source": [
    "y_test=[]\n",
    "test_paths=[]\n",
    "\n",
    "# Read text file containing image paths\n",
    "with open('test.txt', 'r') as f:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    for line in f.readlines():  \n",
    "        image_test = line[:-1].split(' ') \n",
    "\n",
    "        test_paths.append(image_test[0]) #存path\n",
    "        test_labels = image_test[1]\n",
    "\n",
    "        y_test.append(test_labels) #存label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵萃取 Histogram of oriented gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "# 設定 HOG 參數\n",
    "orientations = 9 #HOG特徵的方向數為梯度方向的分組數量\n",
    "pixels_per_cell = (8, 8) #HOG單元的大小，以像素為單位\n",
    "cells_per_block = (3, 3) #組成每個HOG塊的HOG單元的數量\n",
    "\n",
    "def extract_histogram(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) #轉灰階\n",
    "    \n",
    "    hog_features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block)\n",
    "    \n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ee6f2f23ae4494a985ca187a252cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#訓練集的特徵 x_train \n",
    "x_train=[]\n",
    "\n",
    "with tqdm(total=len(train_paths[:10500])) as pbar:\n",
    "    for path in train_paths[:10500]:\n",
    "        try:\n",
    "            features = extract_histogram(path)\n",
    "            x_train.append(features)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        finally:\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63325\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "y_train_cut=y_train[:10500]\n",
    "print(len(y_train_cut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40997b5d6f5248159c00a2de6f91323a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#測試集的特徵 x_test\n",
    "x_test=[]\n",
    "with tqdm(total=len(test_paths)) as pbar:\n",
    "    for path in test_paths:\n",
    "        try:\n",
    "            features = extract_histogram(path)\n",
    "            x_test.append(features)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        finally:\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24870486 0.02180481 0.09653401 ... 0.         0.         0.05040989]\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cut=x_test[:35]\n",
    "y_test_cut=y_test[:35]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 縮小特徵向量的長度\n",
    "length = 10000\n",
    "for i in range(len(x_train)):\n",
    "    features = x_train[i]\n",
    "    features = np.resize(features, (length,))\n",
    "    x_train[i] = features\n",
    "    \n",
    "for i in range(len(x_test_cut)):\n",
    "    features = x_test_cut[i]\n",
    "    features = np.resize(features, (length,))\n",
    "    x_test_cut[i] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#轉成numpy array丟入分類器\n",
    "x_train_np = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = np.array(x_test_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17142857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# 建立 SVM 分類器\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# 訓練分類器\n",
    "svm.fit(x_train_np, y_train_cut)\n",
    "\n",
    "# 進行預測\n",
    "y_pred = svm.predict(x_test_np)\n",
    "\n",
    "# 評估 SVM 分類器的準確率\n",
    "accuracy = svm.score(x_test_np, y_test_cut)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "# 建立 Perceptron 分類器\n",
    "clf = Perceptron()\n",
    "# 訓練分類器\n",
    "clf.fit(x_train_np, y_train_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行預測\n",
    "y_pred_perceptron = clf.predict(x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# 評估 Perceptron 分類器的準確率\n",
    "accuracy_perceptron = clf.score(x_test_np, y_test_cut)\n",
    "print(\"Accuracy:\", accuracy_perceptron)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
